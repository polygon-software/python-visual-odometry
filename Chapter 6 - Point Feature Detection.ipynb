{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Chapter 6 - Point Feature Detection\n",
    "\n",
    "In the last blog article, we've seen how filters can be used to find edges in an image. However, we can also use filters to detect features in an image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Template Matching\n",
    "To illustrate this concept, let's have a look at an image with some simple structures on it. We can copy a small region of our image and use it as a filter / template. When we slide this filter over the image, we'll get a so called *correlation map* that indicates how well the template matched the original picture at each possible position. As we might expect, the point of best correlation is found at the brightest pixel at exactly the position where we extracted the template originally. \n",
    "\n",
    "![Template matching](https://github.com/joelbarmettlerUZH/PyVisualOdometry/raw/master/img/chapter_6/template_matching.png)\n",
    "*Figure 1: Template, Detected template and correlation map. [source](http://rpg.ifi.uzh.ch/docs/teaching/2019/05_feature_detection_1.pdf)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the difference between the template and the image, we can use **cross correlation**, hence the name **cross correlation map**. In normalized cross correlation (NCC), we interpret pixels as vectors and take their cross product divided by the vectors lengths. If the images / vectors are equal, their NCC will be 1. If they are perpendicular, the NCC is 0 - a NCC of -1 suggest that the vectors are exact opposites of each other. \n",
    "While NCC is very robust, the problem with NCC is the computational complexity: It is computationally expensive to calculate NCC, hence it is seldomly used in practice. \n",
    "\n",
    "![Normalized Cross Correlation](https://github.com/joelbarmettlerUZH/PyVisualOdometry/raw/master/img/chapter_6/cross_correlation.png)\n",
    "*Figure 2: Cross correlation formula. [source](http://rpg.ifi.uzh.ch/docs/teaching/2019/05_feature_detection_1.pdf)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative and much faster similarity measurement is the **Sum of Squared Differences** (SSD). SSD, as the name sais, takes the piecewise difference of each pixel value, squares it and sums the results. \n",
    "\n",
    "![Sum of Squared Differences](https://github.com/joelbarmettlerUZH/PyVisualOdometry/raw/master/img/chapter_6/sum_of_squared_differences.png)\n",
    "*Figure 3: Sum of Squared Differences. [source](http://rpg.ifi.uzh.ch/docs/teaching/2019/05_feature_detection_1.pdf)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even cheaper than SSD is the third similarity measurement, called **Sum of Absolute Differences** (SAD). It works like SSD but replaces the square with the absolute function, making it even faster to compute but also less robust. \n",
    "\n",
    "![Sum of Absolute Differences](https://github.com/joelbarmettlerUZH/PyVisualOdometry/raw/master/img/chapter_6/sum_of_absolute_differences.png)\n",
    "*Figure 4: Sum of Absolute Differences. [source](http://rpg.ifi.uzh.ch/docs/teaching/2019/05_feature_detection_1.pdf)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of these difference measurements are prone to intensity variations. To account for this, we can substract the average intensity of the two images (typically caused by additive illumination changes) from each image. We then get what's called **Zero-mean Normalized Cross-Corrleation (ZNCC)**, **Zero-mean Sum of Squared Differences ZSSD)** and **Zero-mean Sum of Absolute Differences ZSAD)**. \n",
    "\n",
    "![Zero-mean similarities](https://github.com/joelbarmettlerUZH/PyVisualOdometry/raw/master/img/chapter_6/zero_mean_similarities.png)\n",
    "*Figure 5: Zero-mean similarities. [source](http://rpg.ifi.uzh.ch/docs/teaching/2019/05_feature_detection_1.pdf)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last error measurement we will have a look in this article is called **Census Transform**. CT maps each pixel in the template patch to either 1 (white) or 0 (black) depending whether it is brighter or darker than the central pixel. Then, it flattens the pixels clockwise into a vector called *bit string*. For a template with dimensions w * w, the resulting bit string vector would be of length w<sup>2</sup> - 1 (minus the central pixel). We can then compare two patches by converting them to a bit string and apply *Hamming distance*, which is the total number of bits that are different. This can be done by performing an XOR operation on both bit strings. \n",
    "This process has the advantage that it is very fast since neither roots nor divisions are required. Since the pixels are compared to the central pixel, the Census Transform is automatically also invariant to monotonic intensity changes. \n",
    "\n",
    "![Census Transform](https://github.com/joelbarmettlerUZH/PyVisualOdometry/raw/master/img/chapter_6/census_transform.png)\n",
    "*Figure 6: Census Transform. [source](http://rpg.ifi.uzh.ch/docs/teaching/2019/05_feature_detection_1.pdf)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While template matching is cool, it's applications are limited to cases where we match on a pixel level. We can't match objects that are just similar to the pixel. We even fail if the overall brightness of the image changed or the rotation / orientation slightly changed. As we compare exact pixel values, we also need to have the exact pixel equivalent to the filter inside the image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point-feature extraction\n",
    "For visual odometry, we want to find and extract features from an image that we can use as templates in another image. This way, we can match features between two images and calculate the relative motion between them. \n",
    "\n",
    "Matching keypoints can also be used for other applications like *Panorama Stitching*, *Object recognition*, *3D reconstruction* or *place recognition*. The problem is always the same: Given one image, we want to match another image of the same scene but taken under different environmental conditions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Panorama stitching\n",
    "As an easy example, let's consider panorama stitching where we have two images taken side by side with a certain overlap, and we want to combine these two images to get a panorama stitch. We can dothis in three steps:\n",
    "- 1.) Find features in both images\n",
    "- 2.) For each feature, find the corresponding pair in the other picture\n",
    "- 3.) Overlap images such that the pairs are aligned as good as possible\n",
    "\n",
    "![Panorama Stitching](https://github.com/joelbarmettlerUZH/PyVisualOdometry/raw/master/img/chapter_6/panorama_stitching.png)\n",
    "*Figure 7: Panorama Stitching. [source](http://rpg.ifi.uzh.ch/docs/teaching/2019/05_feature_detection_1.pdf)*\n",
    "\n",
    "The hard part is to detect the same points independently in both images. We need to have repeateable feature detectors, meaning that we have to re-detect the same features from one images in another. \n",
    "Secondly, we have to find the corresponding features in both images distinctively. To do so, we use descriptors. Descriptors are a description of a pixel and the features around it that uniquely identify the pixel without ambiguity. The descriptor needs to be robust against geometric and illumination changes such that we can find the same descriptor in a different image. Such geometrical changes can be: Translation, Rotation, Scaling and perspective changes. \n",
    "\n",
    "An illumination change is a simple, affine transformation of a pixels value by a constant factor *B*. It can easily be overcome by dividing an image by its average intensity value. \n",
    "Rotation invariation is harder to achieve. We could - for example - always rotate our detected features such that the most dominant lines are horizontal, but we will see more efficient methods later in this article. \n",
    "\n",
    "When we detect features, we usually have two different methods: **corner detection** and **blob detection**. \n",
    "A corner is a region of contrast that changes significantly in two perpendicular directions. It has a very high localization accuracy but are often hard to distinct. In contrast, a Blob is an image region that differs significantly from its neighbours - like a small but dominant pattern. Blogs are less localization accurate than corners but are more distinct and therefore easier to redetect. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moravec Corner Detector\n",
    "Moravec proposed a corner detectoor in which we slide a window into any direction and measure the intensity change. A corner is found when movement into at least two directions results in a large change in intensity. \n",
    "\n",
    "![Moravec Corner Detector](https://github.com/joelbarmettlerUZH/PyVisualOdometry/raw/master/img/chapter_6/moravec_corner_detector.png)\n",
    "*Figure 7: Moravec Corner Detector. [source](http://rpg.ifi.uzh.ch/docs/teaching/2019/05_feature_detection_1.pdf)*\n",
    "\n",
    "We consider a window / patch at position (x,y) and move it to a new location (x+dx, y+dy). we then copute the sum of squared differences between the two patches is large for two different dx and dy pairs, we consider the region (x,y) to contain a corner. \n",
    "\n",
    "![SSD of Moravec Corner Detector](https://github.com/joelbarmettlerUZH/PyVisualOdometry/raw/master/img/chapter_6/ssd_moravec_corner_detector.png)\n",
    "*Figure 8: SSD of Moravec Corner Detector. [source](http://rpg.ifi.uzh.ch/docs/teaching/2019/05_feature_detection_1.pdf)*\n",
    "\n",
    "While Moravecs approach is intuitive to understand, it is also computationally expensive, since for each pixel location, multiple different patch positions have to be calculated and compared to each other. \n",
    "\n",
    "(http://rpg.ifi.uzh.ch/docs/teaching/2019/05_feature_detection_1.pdf)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Harris / Shi-Tomasi Corner Detector\n",
    "Harris implements Moravecs corner detection without the need to physicalls shift the patch window around. It does so by looking at the patch and calculating the gradients, so the derivatives. \n",
    "Harris approximates the shift (dx, dy) using the first taylor expansion: I(x + dx, y + dy) = I(x, y) + I<sub>x</sub>(x, y)dx + I<sub>y</sub>(x, y)dy. \n",
    "The SSD of (dx, dy) can therefore be approximated by taking the sum of squared differences between the images derivative in x-direction (I<sub>x</sub>) and the images derivative in y-direction (I<sub>y</sub>).\n",
    "These derivatives I<sub>x</sub>, I<sub>y</sub> can simply be coputed using a normal Sobel filter.\n",
    "\n",
    "![SSD of Harris Corner Detector](https://github.com/joelbarmettlerUZH/PyVisualOdometry/raw/master/img/chapter_6/ssd_harris_corner_detector.png)\n",
    "*Figure 9: SSD of Harris Corner Detector. [source](http://rpg.ifi.uzh.ch/docs/teaching/2019/05_feature_detection_1.pdf)*\n",
    "\n",
    "To implement this formula, we can write the SSD in matrix form. Note that in the following formula, M is a so called *second moment matrix* containing pixel-wise products of their respective image patch. \n",
    "\n",
    "![SSD of Harris in Matrix representation](https://github.com/joelbarmettlerUZH/PyVisualOdometry/raw/master/img/chapter_6/harris_corner_detector_matrix.png)\n",
    "*Figure 10: SSD of Harris in Matrix representation. [source](http://rpg.ifi.uzh.ch/docs/teaching/2019/05_feature_detection_1.pdf)*\n",
    "\n",
    "To compute M, we use a Box Filter to sum up the patches I<sup>2</sup><sub>x</sub>, I<sup>2</sup><sub>x</sub> and I<sub>x</sub>I<sub>y</sub>. We could also use a box filter to give central pixels more weight. The outcome is a 2x2 matrix.\n",
    "\n",
    "Recap that we can find the eigenvalues l<sub>1</sub>, l<sub>2</sub> and eigenvectors x of a square matri by solving the following equation: \n",
    "\n",
    "**Ax = lx**\n",
    "\n",
    "We can find the eigenvalues by solving **det(A - lI) =  0**\n",
    "\n",
    "Since we deal with a 2x2 matrix, we can easily determine the eigenvalues as and the eigenvectors in three steps:\n",
    "\n",
    "![SSD of Harris in Matrix eigenvalues & eigenvectors](https://github.com/joelbarmettlerUZH/PyVisualOdometry/raw/master/img/chapter_6/harris_corner_detector_eigenvector.png)\n",
    "*Figure 11: SSD of Harris in Matrix eigenvalues & eigenvectors. [source](http://rpg.ifi.uzh.ch/docs/teaching/2019/05_feature_detection_1.pdf)*\n",
    "\n",
    "By solving the the last matrix calculations, we get two eigenvectors x, out of which we can construct a Matrix R = [x1|x2].\n",
    "\n",
    "From the symmetric matrix M, we can do a decomposition as follows, with eigenvalues l<sub>1</sub> and l<sub>2</sub> and eigenvector matrix R:\n",
    "\n",
    "![SSD of Harris in Matrix decomposition](https://github.com/joelbarmettlerUZH/PyVisualOdometry/raw/master/img/chapter_6/harris_corner_detector_matrix_decomposition.png)\n",
    "*Figure 12: SSD of Harris in Matrix decomposition. [source](http://rpg.ifi.uzh.ch/docs/teaching/2019/05_feature_detection_1.pdf)*\n",
    "\n",
    "We can then visualize [dx dy] M [dx dy]' as an ellipse, where the ellipse axis length are exactly the eigenvalues and the axes orientation are the eigenvectors of M. The two eigenvalues will then identify the direction of largest and smalles change of SSD. The longer axis indicates the magnitude (eigenvalue) and direction (eigenvector) of the slowest change in SSD (slow change in contrast), while the shorter axis indicates the fastes change of SSD, so the strongest edge. \n",
    "\n",
    "For a flat region, we will get eigenvalues l<sub>1</sub> and l<sub>2</sub> close to 0. For a horizontal edge, we will get high value for l<sub>2</sub> and a close to zero value for l<sub>1</sub>. Only if we have a corner, both eigenvalues will be significantly larger than 0. For a perfectly perpendicular edge, we will get eigenvectors with orientation of 45Â°. \n",
    "\n",
    "![SSD of Harris in Matrix Eigenvalues](https://github.com/joelbarmettlerUZH/PyVisualOdometry/raw/master/img/chapter_6/harris_corner_detector_eigenvalues.png)\n",
    "*Figure 13: SSD  of Harris in Matrix Eigenvalues. [source](http://rpg.ifi.uzh.ch/docs/teaching/2019/05_feature_detection_1.pdf)*\n",
    "\n",
    "A corner can now be identified by sliding a patch over the image, calculating the eigenvalues of M and checking whether the minimum of the two eigenvalues of M is larger than a certain user-defined threshold. So, if **R = min(l<sub>1</sub>, l<sub>2</sub>) > threshold**, we have identified the patch to contain a corner. If both eigenvalues lay under the threshold, we have a flat region. If only one of the eigenvalues is high enough, the region contains a corner - a horizontal one if only the second eigenvalue l<sub>2</sub> exceeds the threshold, a vertical one otherwise. \n",
    "\n",
    "The function (not Matrix) R is called the *cornerness function*, the detector that uses this function is called **Shi-Tomasi detector**. \n",
    "\n",
    "There's only one problem: Computing eigenvalues is expensive. We can therefore replace the *cornerness-function* by something simpler: \n",
    "**R = l<sub>1</sub> * l<sub>2</sub> - k(l<sub>1</sub> + l<sub>2</sub>)<sup>2</sup> = det(M) - k * trace<sup>2</sup>(M)**\n",
    "k is a magic number and empirically found to fit best if 0.04 <= k <= 0.15. A corner-detector using this function is called **Harris** detector. \n",
    "\n",
    "The following pictures illustrate the different steps in finding Harris features on an example image:\n",
    "\n",
    "![Harris Corner Detection Workflow](https://github.com/joelbarmettlerUZH/PyVisualOdometry/raw/master/img/chapter_6/harris_workflow.png)\n",
    "*Figure 14: Harris Corner Detection Workflow. [source](http://rpg.ifi.uzh.ch/docs/teaching/2019/05_feature_detection_1.pdf)*\n",
    "\n",
    "There is only a small difference between Shi-Tomasi and Harris. In fact, the only result difference is the certainty / robustness to which they detect curner, with Shi-Tomasi being slightly more expensive yet more robust. \n",
    "\n",
    "Both detectors are invariant to rotation changes: The ellipsis roates, but its shape (e.g. its eigenvalues) remain the same, meaning the corner response R is invariant to image rotations. The detectors are also robust to monotonic illumination changes since gthe eigenvalues would change but the local mi**BUT**, it is not invariant to scale, since corners will no longer be detected when the gradient smooths out. In fact, with doubling the image size, the possibility of detecting a corner shirnks down to below 20%. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
